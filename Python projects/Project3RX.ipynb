{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85845e9-dcb2-42ce-ae07-853c8b2a8840",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "Authors: Richard Xiao\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26288dd8-d0aa-4663-9a30-cff5bcf53df2",
   "metadata": {},
   "source": [
    "## Report Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104d114-e283-414f-aca6-2ad05541a544",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33108f-7e42-4233-949e-4f7e4477384d",
   "metadata": {},
   "source": [
    "Supervised learning is a machine learning approach in which datasets are designed to train various algorithms to correctly classify or predict outcomes/decisions accurately based on input data. The point of them is to make predictions in real-time, which allows for faster decision-making and improved efficiency and accuracy. This can save time and resources for organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0cda2-d81e-4de9-8ec9-9a6138e0cbe4",
   "metadata": {},
   "source": [
    "The dataset I used for this project details employee attrition from a sample of IBM employees. This dataset was obtained from Kaggle. This is a dataset that comes from Kaggle. The goal here is to classify employee attrition based on 7 indicators which are gender, department, age, traveling distance, gender, marital status and monthly income. Do certian indicator variables play a part in influencing the chance of an employee leaving and which classification models will be the best in determining the prediction of employee attrition based on different indicator variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409fc60-1ef2-4e6b-9ff0-1aa63d78c47e",
   "metadata": {},
   "source": [
    "## Splitting the Data,Metrics, and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e170d6-5e44-41dd-b478-8cfbb7b5bb56",
   "metadata": {},
   "source": [
    "For my models, the metrics I want to use are log loss and accuracy for the classification models. The advantage of using accuracy is its' overall simplicity, meaning that it's easy to understand and implement. It's a great metric for balanced datasets, meaning that the number of positive and negative cases is roughly equal. However, the problem for accuracy is that if the dataset isn't balanced, then it's no longer a useful metric to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae311a-2da5-43fb-8164-1116f495d2a7",
   "metadata": {},
   "source": [
    "For log loss, log loss is easy to optimize and is useful to compare models on their probabilistic outcome. It also penalizes wrong predictions quite strongly, meaning that it is more sensitive to the uncertainty of the model's predictions. However, log loss is sensitive to outliers, which can lead to misleading results or even overfitting. Log losses are affected by imbalanced classes since they can be biased towards the majority class which means that they can perform poorly on certain parts of a data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb19351-5563-4309-a553-c6192513175e",
   "metadata": {},
   "source": [
    "There are a number of reasons why we want to split our data into training and test sets. We want to evaluate the performance of a model on unseen data. To do this, we train one subset of the data, which is the training dataset, and evaluate its' performance on another subset of the data, the test set, which will give us an estimate on how well the model will generalize to new data. Evaluating the model on  the test set can prevent overfitting since we can compare the test and training error, which can help us to detect any overfitting. Keeping the test and training data separate can prevent any data leakage, thus keeping the model unbiased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b756a3-8181-4763-b21b-816cf7ef9668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c2697-5eb6-4276-a772-f2afe756b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "spark = SparkSession.builder.master('local[*]').appName('my_app').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f11a7b0-b15a-42f0-a827-313907f2285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer, StringIndexer,Binarizer,VectorAssembler, PolynomialExpansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420b7ae-7825-41f1-a3fd-bd790bd0f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_data = spark.read.load(\"employee_attrition.csv\",\n",
    "                            format = \"csv\",\n",
    "                            sep = \",\",\n",
    "                            inferSchema = \"true\",\n",
    "                            header = \"true\")\n",
    "\n",
    "employee_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d366d-7e5a-4931-b45b-353e48087a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_employee = employee_data.select(['Age','Attrition','MaritalStatus','Gender','OverTime','DistanceFromHome','MonthlyIncome','Department'])\n",
    "select_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b1e457-ce42-492f-b56a-01f7e6237fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d43b6-bb91-4ad8-a4bb-635954c00c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3842075-ddaa-4e23-9657-2298effbeab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc2cd3-c6c8-472e-bf4d-11ed27ed9cc8",
   "metadata": {},
   "source": [
    "## Splitting testing and training data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2865c64-f180-4110-83cc-a023e204fdf2",
   "metadata": {},
   "source": [
    "Decided to use 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea967b-ca73-43ce-ad18-0093298cf4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = select_employee.randomSplit([0.8,0.2], seed = 1)\n",
    "print(train.count(), test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c28ea6c-5350-4d3d-ad2c-91002ef8223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19078b-e5fb-4ce1-88e4-ca0b2d1537a7",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f5df6-611b-4f97-b663-546cd814bd9b",
   "metadata": {},
   "source": [
    "Here is where I proceed to start testing my 5 models. The 5 I've chosen are elastic net, lasso, decision trees, random forest and naive baye."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23ff7c-4dd9-4246-94ab-2e4f99054082",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe67f7-270e-4ef9-8ba4-bc2f5ffecaf7",
   "metadata": {},
   "source": [
    "Elastic Net logistic regression is a type of regression that combines both lasso and ridge logistic regression models. It performs variable selection and regularization by adding a penalty term to the loss function, as well as adding a regularization term that controls the magnitude of the coefficients. This improves the stability of the model and reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74716e6e-9ce5-4e3b-a005-c2a219d0b80a",
   "metadata": {},
   "source": [
    "Code below declares the indexer transformation to convert categorical variables into numeric variables to be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b6d0a-5c9d-4f75-9d30-1080fd0777fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition_indexer = StringIndexer(inputCols = [\"Attrition\",\"Department\",\"Gender\",\"OverTime\",\"MaritalStatus\"], outputCols = [\"attrition_numeric\",\"Department_numeric\",\"gender_numeric\",\"overtime_numeric\",\"maritalstatus_numeric\"])\n",
    "attritionTrans = attrition_indexer.fit(select_employee)\n",
    "attritionTrans.transform(select_employee).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288f3378-f595-4b26-92c6-6ded225c5ee0",
   "metadata": {},
   "source": [
    "Binarizer transformation code is below to use as binary variables to be tested on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1547c0-4ee2-422e-a462-4789d134ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryattritionTrans = Binarizer(threshold = 0.5, inputCols = [\"attrition_numeric\", \"Department_numeric\",\"gender_numeric\",\"overtime_numeric\",\"maritalstatus_numeric\"], outputCols = [\"attrition_indicator\",\"department_indicator\",\"gender_indicator\",\"overtime_indicator\",\"maritalstatus_indicator\"])\n",
    "binaryattritionTrans.transform(attritionTrans.transform(select_employee)).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c8adaa-a179-4108-a3c7-446181133c8f",
   "metadata": {},
   "source": [
    "Sql transformer is used to select all the indicator variables as well as declaring the attrition as the label, or response variable. Log transformation is done on monthly income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76976860-42e9-47d6-bdb3-e63f83680cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "                SELECT department_indicator,overtime_indicator,maritalstatus_indicator, Age, gender_indicator,DistanceFromHome,log(MonthlyIncome) as log_monthly_income, attrition_numeric as label FROM __THIS__\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da00ccc-50fd-4611-b436-ed4e7d39af2e",
   "metadata": {},
   "source": [
    "Assembler transformation code below. Everything is ready to be set for model testing into pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31675b56-3fc0-4b82-9825-072e2b14a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = [\"department_indicator\", \"Age\",\"gender_indicator\",\"overtime_indicator\",\"maritalstatus_indicator\",\"DistanceFromHome\"], outputCol = \"features\", handleInvalid = 'keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da4c48-af7f-4cf7-815d-2a0886224d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719064a7-1129-4058-ad15-27ab3f65ac27",
   "metadata": {},
   "source": [
    "For the code below, the logistic regression object was declared. The regParam option controls the strength of the regularization term in the elastic net model. It represents the shrinkage applied to the coefficients of the model. Multiple values were used to find the optimal value to find the best performing model. Values  0.01 and 0.1 are the boundaries, since 0.01 is a good starting value to provide regularization without overly penalizing the coefficients. It shouldn't be set too low or too high so it wouldn't overfit or underfit the model respectively.\n",
    "\n",
    "Next, the elasticNetParam grid is added to control the ratio between L1 and L2 penalties in the elastic net model. In this case, the ElasticNetParam is set between a value of 0 and 1, where 0 represents L2 regularization and 1 is L1 regularization. Similar to the regParam option, it serves as a tuning hyperparameter to find the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48101247-d16d-4d67-99e1-9e2835cef01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5]) \\\n",
    "    .build()\n",
    "pipeline = Pipeline(stages = [attritionTrans, binaryattritionTrans, sqlTrans, assembler,lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940238f-33dc-47a3-a184-21109afada7e",
   "metadata": {},
   "source": [
    "Declare evaluator variable to dictate the metric I want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7409c-83b9-4bbb-9e21-21129827d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fde1b8-cfc9-4766-91f9-5d46bda87b0f",
   "metadata": {},
   "source": [
    "Cross validator variable is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f7e5f-5445-4f44-a6f7-2bd1bb3ba513",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator= evaluator,\n",
    "                    numFolds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6477f280-dfcd-469a-9496-930023b8a747",
   "metadata": {},
   "source": [
    "Training model is fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3605f0-1a72-4779-9683-f5ea348c8e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825101f4-b7c6-478d-8299-7a418d08fb28",
   "metadata": {},
   "source": [
    "Best model method is used to refer to the best model that is selected based on evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d4394d-3655-4c92-beaa-110f4ccfb586",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68721a11-b2ae-402f-8f64-9bbfae0b0067",
   "metadata": {},
   "source": [
    "Code below shows test data set being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a125cd-d8b3-4a73-8df3-7c8a9c81840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bestModel.transform(test)\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617bdeb5-b68e-482a-bdd3-7d97e00f42a9",
   "metadata": {},
   "source": [
    "Accuracy results code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce1731-9ba4-420d-ad6e-70a0dc3ba161",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3bc6e-76d1-4ec4-841c-491544cedde9",
   "metadata": {},
   "source": [
    "New evaluator is declared for log loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53256f2f-4059-4605-8a06-f38f48b29c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"logLoss\")\n",
    "logLoss = evaluator.evaluate(predictions)\n",
    "print(logLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cbee9-6756-4334-89ff-8a4b40adbd85",
   "metadata": {},
   "source": [
    "Note that this process is repeated 4 more times. Only difference is the type of classifier differs for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2d44e-871e-44d2-bbb3-14c45fa67325",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180654a-2c93-4c38-aa21-ca021409ab1f",
   "metadata": {},
   "source": [
    "The idea of lasso regression for this dataset is a regularization technique to prevent overfitting and improve the predictive performance of logistic regression models. It adds a penalty term to the logistic regression function that allows the model to select the most important predictors and eliminate any extraneous variables. This is to reduce the variance of the model and improve performance by shrinking the coefficients of the less important variables towards zero. This has an advantage towards datasets where overfitting is likely or where there is a large number of variables. In this case, I don't think it will make that much of a difference since there isn't a large amount of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2d5f4-1e28-45ee-a235-f1caddd105a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition_indexer = StringIndexer(inputCols = [\"Attrition\",\"Department\",\"Gender\",\"OverTime\",\"MaritalStatus\"], outputCols = [\"attrition_numeric\",\"Department_numeric\",\"gender_numeric\",\"overtime_numeric\",\"maritalstatus_numeric\"])\n",
    "attritionTrans = attrition_indexer.fit(select_employee)\n",
    "attritionTrans.transform(select_employee).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7bc1f8-24d5-432d-9891-89e1ae7ba4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryattritionTrans = Binarizer(threshold = 0.5, inputCols = [\"attrition_numeric\", \"Department_numeric\",\"gender_numeric\",\"overtime_numeric\",\"maritalstatus_numeric\"], outputCols = [\"attrition_indicator\",\"department_indicator\",\"gender_indicator\",\"overtime_indicator\",\"maritalstatus_indicator\"])\n",
    "binaryattritionTrans.transform(attritionTrans.transform(select_employee)).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa56ee7-784b-44b7-952c-7d75b2074801",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "                SELECT department_indicator,overtime_indicator,maritalstatus_indicator, Age, gender_indicator,DistanceFromHome,log(MonthlyIncome) as log_monthly_income, attrition_numeric as label FROM __THIS__\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a69e46-cb27-4c46-b9cb-2e0714a13ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = [\"department_indicator\", \"Age\",\"gender_indicator\",\"overtime_indicator\",\"maritalstatus_indicator\",\"DistanceFromHome\"], outputCol = \"features\", handleInvalid = 'keep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88609cf8-7a8f-43aa-9272-cbaabd9f51fc",
   "metadata": {},
   "source": [
    "For the two lines of code below, the elasticNetParam value was set to 1 to represent L1 regularization since lasso models use L1 regularization. The regParam serves the same purpose as the regParam option for my elastic net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01646bf3-ff04-4e20-b287-c5122ded52a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", elasticNetParam=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b4528-85e6-4fb3-85a7-a48cb1c7e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lasso.regParam, [0.01, 1.0]) \\\n",
    "    .build()\n",
    "pipeline = Pipeline(stages = [attritionTrans, binaryattritionTrans, sqlTrans, assembler,lasso])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53454c02-0421-4cbd-9a92-891996ab9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ad19e-7418-4bb8-ade0-d679e72e0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=lasso,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dab2de-8ec2-439e-9dec-1750494623b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50735d47-00f2-420d-b0e5-b0e13d35c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9371a-7926-4bdb-9ec4-8bbba619259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bestModel.transform(test)\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08785d0-5fde-4bf7-b1bf-2c0b2c922d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa85553-c48e-40c7-bd20-f18753f7f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc10a3a-9429-49b2-b0e2-69b2254b7750",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"logLoss\")\n",
    "logLoss = evaluator.evaluate(predictions)\n",
    "print(logLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6216f76-5d0d-45b3-859c-b8df08d5f9d5",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f01a0-6245-4826-941a-918ae3de4d16",
   "metadata": {},
   "source": [
    "Random forest is an ensemble mmodel which combines the predictions of multiple decision trees to improve the accuracy and robustness of the model. It selects a subset of the training data and features. Next, they construct a decision tree based off these and repeat this process to a predefined number of trees. These predictions are combined to make a final prediction. This method reduces overfitting and improves the generalization performance of the model. The combination of multiple decision trees also helps to reduce the variance of the model and improve its robustness. It's similar to bagging although it doesn't use all of the predictors in case a strong predictor exists which can skew the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de576c-3b6e-4f21-9d5b-ebc4f498a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition_indexer = StringIndexer(inputCols = [\"Attrition\",\"Department\",\"Gender\",\"OverTime\",\"MaritalStatus\"], outputCols = [\"attrition_numeric\",\"Department_numeric\",\"gender_numeric\",\"overtime_numeric\",\"maritalstatus_numeric\"])\n",
    "attritionTrans = attrition_indexer.fit(select_employee)\n",
    "attritionTrans.transform(select_employee).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd01bb-bb2b-441d-b1f1-a49e5c73ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryattritionTrans = Binarizer(threshold = 0.5, inputCols = [\"attrition_numeric\", \"Department_numeric\",\"gender_numeric\",\"overtime_numeric\",\"maritalstatus_numeric\"], outputCols = [\"attrition_indicator\",\"department_indicator\",\"gender_indicator\",\"overtime_indicator\",\"maritalstatus_indicator\"])\n",
    "binaryattritionTrans.transform(attritionTrans.transform(select_employee)).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ede9c-2161-4885-ba6f-2437fc33dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "                SELECT department_indicator,overtime_indicator,maritalstatus_indicator, Age, gender_indicator,DistanceFromHome,log(MonthlyIncome) as log_monthly_income, attrition_numeric as label FROM __THIS__\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff3004-aeb8-4b14-8763-7b7344555d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = [\"department_indicator\", \"Age\",\"gender_indicator\",\"overtime_indicator\",\"maritalstatus_indicator\",\"DistanceFromHome\"], outputCol = \"features\", handleInvalid = 'keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b33e4d-2e5e-4df2-bccb-efdeea53404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4377ed03-aa92-4b32-85f0-6a7a0c16566e",
   "metadata": {},
   "source": [
    "For my random forests, I declare numTrees for my randomforestclassifier and paramgridbuilder to indicate how big my forest is. The maxDepth is a tuning parameter to dictate how deep the tree will be when you take a random subset from the data. MaxDepth is set to 5 since it shouldn't exceed the number of indicators present in the model. The minInstancesPerNode option indicates the minimum number of samples required to split a node further in the tree. A higher value for minInstancesPerNode results in smaller trees with fewer splits which can result in lower variance in the model. However, a very high number can result in a higher bias in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03c0d1-81f6-47d7-b602-fce3a0097272",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees = 50, maxDepth = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b2b18b-9c13-4771-ab7b-e8a6b4e36bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [20, 30, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [1, 3, 5]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .build()\n",
    "pipeline = Pipeline(stages = [attritionTrans, binaryattritionTrans, sqlTrans, assembler,rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d2612-8503-44d4-8324-7b10e21eafd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06f572a-a4c5-42ae-af72-367389ecfd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e05caf-5b10-4fbb-a58d-8b97119a1498",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed302c0-f6f6-4e11-b1cf-cbde5c2e00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cvModel.transform(test)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b0fe1-558d-4b73-bd12-1d024c6777f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e6df84-5891-48f9-931e-953f4f085641",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"logLoss\")\n",
    "logloss = evaluator.evaluate(predictions)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a96b37-7ca4-4c4d-8024-984a62fa4d6e",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb268db-56db-4859-9a34-8c69c96c3c92",
   "metadata": {},
   "source": [
    "Decision trees make choices based on different criteria to help classify and predict outcomes. It starts off with a single node(or point) and then branches out into different paths. Each branch of the tree represents a decision or choice, and each leaf node represents a possible outcome. The tree is constructed by  splitting the data based on different criteria to minimize the error or to maximize information gained at each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b063729-2ae0-4c29-b881-7841ad29546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition_indexer = StringIndexer(inputCols = [\"Attrition\",\"Department\",\"Gender\",\"OverTime\",\"MaritalStatus\"], outputCols = [\"attrition_numeric\",\"Department_numeric\",\"gender_numeric\",\"overtime_numeric\",\"maritalstatus_numeric\"])\n",
    "attritionTrans = attrition_indexer.fit(select_employee)\n",
    "attritionTrans.transform(select_employee).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b4b92-38f5-46a5-ac42-62e0bf5379f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryattritionTrans = Binarizer(threshold = 0.5, inputCols = [\"attrition_numeric\", \"Department_numeric\",\"gender_numeric\",\"overtime_numeric\",\"maritalstatus_numeric\"], outputCols = [\"attrition_indicator\",\"department_indicator\",\"gender_indicator\",\"overtime_indicator\",\"maritalstatus_indicator\"])\n",
    "binaryattritionTrans.transform(attritionTrans.transform(select_employee)).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326abd2-ccc5-482b-bf6c-0f88a69bbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "                SELECT department_indicator,overtime_indicator,maritalstatus_indicator, Age, gender_indicator,DistanceFromHome,log(MonthlyIncome) as log_monthly_income, attrition_numeric as label FROM __THIS__\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91cab93-5d31-4104-adeb-b2c9c25df108",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = [\"department_indicator\", \"Age\",\"gender_indicator\",\"overtime_indicator\",\"maritalstatus_indicator\",\"DistanceFromHome\"], outputCol = \"features\", handleInvalid = 'keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a18b6-b30c-4fd2-8163-776db10bddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71682a84-cd29-4a1a-bd20-8ea81faf8318",
   "metadata": {},
   "source": [
    "Code below declares the decision tree classifier and param grid. Used the same parameters that I used for the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eba7f6-15fe-4bd2-81c7-09aab11424ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(maxDepth=2, labelCol=\"label\", leafCol=\"leafId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17fa97-fcbd-4f9b-b284-15746242f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [1,3,5]) \\\n",
    "    .addGrid(dt.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .build()\n",
    "pipeline = Pipeline(stages = [attritionTrans, binaryattritionTrans, sqlTrans, assembler,dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45272c58-67f8-428a-88a7-15916a5b06e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol='label') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f553db6-002b-49b9-bba8-c7bff7d1eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008137c5-5bd4-4368-bd14-975649a56e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8c643-3d5b-4fbf-b8a6-2f9721329964",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d16f95-a40e-4f1f-a6aa-7c68416857db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bestModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350c0d8-5b9b-470d-bad7-72017ffdc775",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded313d-0c82-49b4-b8c6-3ed45d837618",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cvModel.transform(test)\n",
    "predictions.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401d3bd-4a27-4059-9227-5e0362df80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"logLoss\")\n",
    "log_loss = evaluator.evaluate(predictions)\n",
    "print(\"Log loss: \", log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cecc5a1-6c3e-4dcc-b23b-feb28ec53af4",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77d0f8-30e0-4096-8288-b7315ef011f5",
   "metadata": {},
   "source": [
    "Naive Bayes classifies data into different categories or classes based on the probabilities of the features. Like the name suggests, it is based on Bayes theorem and it assumes that the features used for classification are independent of each other. It calculates the probability of each feature given a particular class, and then multiplying all these probabilities to get the probability of that class given the features. One of the key advantages of Naive Bayes is its simplicity and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db499011-76b3-442e-abd5-605d8e483f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition_indexer = StringIndexer(inputCols = [\"Attrition\",\"Department\",\"Gender\",\"OverTime\",\"MaritalStatus\"], outputCols = [\"attrition_numeric\",\"Department_numeric\",\"gender_numeric\",\"overtime_numeric\",\"maritalstatus_numeric\"])\n",
    "attritionTrans = attrition_indexer.fit(select_employee)\n",
    "attritionTrans.transform(select_employee).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19dc7a-0ddb-4dc1-ab23-d5b0b32be992",
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryattritionTrans = Binarizer(threshold = 0.5, inputCols = [\"attrition_numeric\", \"Department_numeric\",\"gender_numeric\",\"overtime_numeric\",\"maritalstatus_numeric\"], outputCols = [\"attrition_indicator\",\"department_indicator\",\"gender_indicator\",\"overtime_indicator\",\"maritalstatus_indicator\"])\n",
    "binaryattritionTrans.transform(attritionTrans.transform(select_employee)).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5f225-f2f7-4fec-a8a5-ffaf9ec83bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "                SELECT department_indicator,overtime_indicator,maritalstatus_indicator, Age, gender_indicator,DistanceFromHome,log(MonthlyIncome) as log_monthly_income, attrition_numeric as label FROM __THIS__\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801f723-3bde-494b-b39d-58b7ce04a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = [\"department_indicator\", \"Age\",\"gender_indicator\",\"overtime_indicator\",\"maritalstatus_indicator\",\"DistanceFromHome\"], outputCol = \"features\", handleInvalid = 'keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c284875-4885-4055-b3b0-cda1cf3ce898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7323fd8-54d6-4520-b21b-8ff4b2d2f0d0",
   "metadata": {},
   "source": [
    "The code below declares the naive bayes object as well as setting up the paramgrid for the Naive Bayes model. The smoothing hyperparameter controls the regularization applied to the model. Smoothing is used to tackle the problem of zero probabilities in the training data, which occurs if a feature hasn't been seen in the training dataset. The coefficients in the smoothing option represents the alpha and used to adjust the probability estimates of the model. Larger value means more smoothing which can improve the robustness and generalization of the model. Multiple values are included so the best parameter is picked during cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c825a-5222-453e-a12b-24796a38c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c5c49-b2a4-4c51-ab7f-3902bbf9aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]) \\\n",
    "    .build()\n",
    "pipeline = Pipeline(stages = [attritionTrans, binaryattritionTrans, sqlTrans, assembler,nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855f69a-04f2-44d8-a92c-3316b5a38ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381f659-bed4-4086-8869-67aa1f85162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5e7e4-4ab6-40a6-9882-c59d491a6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161b2cb-6425-438b-8d66-b8e09d14acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fb5cb-ad27-46ec-8026-105d2ca1a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bestModel.transform(test)\n",
    "predictions.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3635de7-a636-423e-82e0-c63afd800c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9bf92b-7d30-4c40-8d16-5b9ca264633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"logLoss\")\n",
    "log_loss = evaluator.evaluate(predictions)\n",
    "print(\"Log loss: \", log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d170837-e589-4fd7-8bc2-6f07e0f1b7ac",
   "metadata": {},
   "source": [
    "## Model Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594fb23f-d6f3-41b9-95d2-83c7ec04a1bb",
   "metadata": {},
   "source": [
    "According to each of the outputs from these 5 models, the elastic net and lasso models had the highest accuracy of 0.84 and lowest log loss metric of 0.3926. Note that both these models had the exact same accuracy and log loss values. Random forest model has the next highest accuracy value of about 0.76 and a log loss value about of 0.40. Naive Bayes has an accuracy value of about 0.71 and log loss of 0.64. Finally, the decision tree model has an accuracy of 0.53 and log loss of 4.9. Based on these observations, elastic net and lasso models are the best models since they outperformed the other models. Random forest model is the next best one with a lower accuracy although its' log loss value is very close to the elastic net and lasso models. Following up after is Naive Bayes and here is where there is a significant drop in model quality. There is a significant increase in log loss. Finally, decision tree is by far the worst model with pretty horrible accuracy(0.5!) and higher log loss value(although it is lower than the naive bayes value.) \n",
    "\n",
    "It is also worth noting that adding more indicators significantly increased the accuracy/decreased log loss for the random forest, decision tree and Naive Bayes model. Adding in overtime, marital status and travel distance made the models perform better so based on these observations, it is possible that these three indicators could be important in determining the chance of an employee leaving in the context of these three models. However, these indicators did not really impact these metrics for the elastic net and lasso models.\n",
    "\n",
    "In conclusion, the elastic net and lasso models would serve as the best models in classifying employee attrition based off the seven indicator variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
